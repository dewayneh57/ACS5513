# -*- coding: utf-8 -*-
"""ACS 5513 - Petabyte Pirates Project Deliverable 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10mTYa4TVJd4LEYcQDxOxoOGWro5ZIR_D

# Ames Housing - Project Deliverable 1
**ACS-5513 - Applied Machine Learning**

**Dr. Neelam Dwivedi**

**Petabyte Pirates (Team A)**

**Source:** https://www.kaggle.com/datasets/prevek18/ames-housing-dataset

## Initial Imports and Data Sourcing
"""

# imports
import numpy as np
import pandas as pd
import kagglehub
from kagglehub import KaggleDatasetAdapter
import matplotlib.pyplot as plt
from scipy import stats

# options
pd.set_option("display.max_columns", None)

file_path = "AmesHousing.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "prevek18/ames-housing-dataset",
  file_path,
)

print(df.shape)
df.head(20)

df.info(verbose=False)
df.describe().T[['mean','std','min','max']]

"""## Data Processing and EDA

### Process Missing and Harcoded NaN Values

**Result:** All hardcoded "NaN" string values have been properly typed with nulls. Features with > 60% missing values have been marked for deletion.
"""

# Convert 'NaN' string literals to missing values prior to missing values analysis.
object_cols = df.select_dtypes(include='object').columns
df[object_cols] = df[object_cols].replace("NaN", np.nan)

# Check the percentage of values that are missing.
# Heuristic: if > 60% of values are missing, drop these columns.
miss = df.isna().mean().sort_values(ascending=False)
miss.head(25)

"""## Normal Distribution Testing

**Result**: The `SalePrice` distribution is highly right-skewed (1.74), which violates linear regression assumptions. We first removed outliers (Greater than, or less than, 1.5 times the Inter-Quartile Range) which signficantly reduced the skewedness (0.67). We attempted to apply a natural log transformation (`log1p(SalePrice)`) to normalize the distribution and stabilize variance, which resulted in a left-skewed distribution but a slightly improved skewedness value (-0.52).

When evaluating the D'Agostino-Pearson test for normality, the original `SalePrice` with outliers removed produced a better result statistic (176.7) than the log-transformed data (236.6).

For the sake of this deliverable, all analysis and early data modeling moving forward will use the original sale price with outliers removed, and the team can revisit additional transformations during the modeling phase.
"""

# Normal Distribution Test for Raw Data

statistic, p_value = stats.normaltest(df['SalePrice'])
print(f"Normaltest (D'Agostino-Pearson): Statistic={statistic:.4f}, p-value={p_value:.4f}")

plt.figure(figsize=(8,4))
plt.hist(df['SalePrice'], bins=40, color='blue', edgecolor='black')
plt.title('Distribution of SalePrice')
plt.xlabel('Sale Price ($)')
plt.ylabel('Frequency')
plt.show()

print("Skewness:", df['SalePrice'].skew().round(2))

# Remove outliers
Q1 = df['SalePrice'].quantile(0.25)
Q3 = df['SalePrice'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - (1.5 * IQR)
upper_bound = Q3 + (1.5 * IQR)
print("Lower Bound:", lower_bound)
print("Upper Bound:", upper_bound)

outliers = df[(df['SalePrice'] < lower_bound) | (df['SalePrice'] > upper_bound)]
print("Number of outliers:", len(outliers))

df.drop(outliers.index, inplace=True)

plt.figure(figsize=(8,4))
plt.hist(df['SalePrice'], bins=40, color='blue', edgecolor='black')
plt.title('Distribution of SalePrice')
plt.xlabel('Sale Price ($)')
plt.ylabel('Frequency')
plt.show()

print("Skewness:", df['SalePrice'].skew().round(2))

# Re-try Normal Distribution Test for Raw Data (removing outliers)

statistic, p_value = stats.normaltest(df['SalePrice'])
print(f"Normaltest (D'Agostino-Pearson): Statistic={statistic:.4f}, p-value={p_value:.4f}")

# Apply Log Transformation
log_df = df.copy()
log_df['SalePrice'] = np.log1p(log_df['SalePrice'])

plt.figure(figsize=(8,4))
plt.hist(log_df['SalePrice'], bins=40, color='blue', edgecolor='black')
plt.title('Distribution of Log(SalePrice)')
plt.xlabel('Log(Sale Price)')
plt.ylabel('Frequency')
plt.show()

print("Skewness:", log_df['SalePrice'].skew().round(2))

# Normal Distribution Test for Log Data

statistic, p_value = stats.normaltest(log_df['SalePrice'])
print(f"Normaltest (D'Agostino-Pearson): Statistic={statistic:.4f}, p-value={p_value:.4f}")

"""## Time Series Data Analysis

**Result**: While there is clear seasonality in the number of homes sold, there is a moderate pull in `SalePrice` with regards to `Month Sold`. The team will consider whether to incorporate seasonal forecasting, but for now is focused on `SalePrice` prediction.
"""

# Descriptive Statistics by Year and Month Sold

fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 16))

# Bar chart showing the number of houses sold by Yr Sold, sorted by Yr Sold
sales_by_year = df['Yr Sold'].value_counts().sort_index()
sales_by_year.plot(kind='bar', ax=axes[0, 0])
axes[0, 0].set_title('Number of Houses Sold by Year Sold')
axes[0, 0].set_xlabel('Year Sold')
axes[0, 0].set_ylabel('Number of Houses')
axes[0, 0].tick_params(axis='x', rotation=45)


# Line chart plotting the number of houses sold by DATE(Yr Sold, Mo Sold, 01) sorted by this date
df['SaleDate'] = pd.to_datetime(df['Yr Sold'].astype(str) + '-' + df['Mo Sold'].astype(str) + '-01')
sales_by_date = df['SaleDate'].value_counts().sort_index()
sales_by_date.plot(kind='line', ax=axes[0, 1])
axes[0, 1].set_title('Number of Houses Sold by Date')
axes[0, 1].set_xlabel('Date')
axes[0, 1].set_ylabel('Number of Houses')
axes[0, 1].tick_params(axis='x', rotation=45)


# Dual Bar chart showing the median and average SalePrice values by year
median_price_by_year = df.groupby('Yr Sold')['SalePrice'].median()
average_price_by_year = df.groupby('Yr Sold')['SalePrice'].mean()
price_by_year = pd.DataFrame({'Median': median_price_by_year, 'Average': average_price_by_year})
price_by_year.plot(kind='bar', ax=axes[1, 0])
axes[1, 0].set_title('Median and Average SalePrice by Year')
axes[1, 0].set_xlabel('Year')
axes[1, 0].set_ylabel('Sale Price ($)')
axes[1, 0].tick_params(axis='x', rotation=45)

# Line chart plotting the 3-month moving average for median sale price (blue)
# and the average house price (red) sold by DATE(Yr Sold, Mo Sold, 01) sorted by this date
median_price_by_date = df.groupby('SaleDate')['SalePrice'].median().rolling(window=3).mean()
average_price_by_date = df.groupby('SaleDate')['SalePrice'].mean().rolling(window=3).mean()
median_price_by_date.plot(kind='line', ax=axes[1, 1], color='blue')
average_price_by_date.plot(kind='line', ax=axes[1, 1], color='red')
axes[1, 1].set_title('3-Month Moving Average of Median and Average SalePrice by Date')
axes[1, 1].set_xlabel('Date')
axes[1, 1].set_ylabel('Sale Price ($)')
axes[1, 1].tick_params(axis='x', rotation=45)

# Dual Bar chart showing the median and average SalePrice values by Quarter
median_price_by_quarter = df.groupby(df['SaleDate'].dt.to_period('Q'))['SalePrice'].median()
average_price_by_quarter = df.groupby(df['SaleDate'].dt.to_period('Q'))['SalePrice'].mean()
price_by_quarter = pd.DataFrame({'Median': median_price_by_quarter, 'Average': average_price_by_quarter})
price_by_quarter.plot(kind='bar', ax=axes[2, 0])
axes[2, 0].set_title('Median and Average SalePrice by Quarter')
axes[2, 0].set_xlabel('Quarter')
axes[2, 0].set_ylabel('Sale Price ($)')
axes[2, 0].tick_params(axis='x', rotation=45)

# Line chart plotting the median sale price (blue) and the average sale price
# (red) sold by Quarter, sorted by this date
price_by_quarter = pd.DataFrame({'Median': median_price_by_quarter, 'Average': average_price_by_quarter})
price_by_quarter.plot(kind='line', ax=axes[2, 1])
axes[2, 1].set_title('Median and Average SalePrice by Quarter')
axes[2, 1].set_xlabel('Quarter')
axes[2, 1].set_ylabel('Sale Price ($)')
axes[2, 1].tick_params(axis='x', rotation=45)

# Dual Bar chart showing the median and average SalePrice values by Mo Sold to further explore potential seasonal effects on price.
# We are excluding 2010 due to data being capped in July of that year.
median_price_by_month = df[df['Yr Sold'] != 2010].groupby('Mo Sold')['SalePrice'].median()
average_price_by_month = df[df['Yr Sold'] != 2010].groupby('Mo Sold')['SalePrice'].mean()
price_by_month = pd.DataFrame({'Median': median_price_by_month, 'Average': average_price_by_month})
price_by_month.plot(kind='bar', ax=axes[3, 0])
axes[3, 0].set_title('Median and Average SalePrice by Month')
axes[3, 0].set_xlabel('Month')
axes[3, 0].set_ylabel('Sale Price ($)')
axes[3, 0].tick_params(axis='x', rotation=45)

# Line chart plotting the median house price (blue) and the average house price (red) sold by Mo Sold, sorted ascending
# Again, we are excluding 2010 due to data being capped in July of that year.
price_by_month = pd.DataFrame({'Median': median_price_by_month, 'Average': average_price_by_month})
price_by_month.plot(kind='line', ax=axes[3, 1])
axes[3, 1].set_title('Median and Average SalePrice by Month')
axes[3, 1].set_xlabel('Month')
axes[3, 1].set_ylabel('Sale Price ($)')
axes[3, 1].tick_params(axis='x', rotation=45)

fig.suptitle('Descriptive Statistics by Year', fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()

"""## Descriptive Statistics by Year and Month Sold Analysis

### Number of Houses Sold
* For the first four years in the dataset, there is no clear increase or decrease in the number of houses sold.
* In 2010, there is a distinct dropoff, but this is due to the dataset being capped in July of that year.
* **Seasonality** - there is a *very* strong seasonal trend in the data, with sales dropping off in Q4 each and every year.
   * While our initial goal is to predict the Sales Price of a home with the given characteristics, we are also **extremely curious** what modeling sales using multi-variate methods to this seasonal time series dataset would look like.

## Categorical Exploratory Data Analysis
"""

focus_categories = [
    'Neighborhood', 'House Style', 'Bldg Type', 'Sale Condition',
    'Sale Type', 'Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond',
    'Kitchen Qual', 'Functional', 'Heating QC', 'Bsmt Qual', 'Bsmt Cond',
    'Foundation', 'BsmtFin Type 1', 'BsmtFin Type 2', 'MS SubClass',
    'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Exter Qual', 'Exter Cond',
    'Roof Style', 'Lot Shape', 'Lot Config', 'MS Zoning', 'Condition 1',
    'Bsmt Exposure', 'Fireplace Qu'
]

fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(22, 44))
axes = axes.flatten()

for i, col in enumerate(focus_categories):
    ax = axes[i]
    df.boxplot(column='SalePrice', by=col, ax=ax, rot=45, grid=False)
    ax.set_title(f'{col}')
    ax.set_ylabel('SalePrice')

for j in range(len(focus_categories), len(axes)):
    fig.delaxes(axes[j])

fig.suptitle('SalePrice Distributions by Categorical Feature', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.97])
plt.show()

"""### Box Plot Interpretation

#### Neighborhood  
This is a prime candidate for one hot encoding as there is clear separation in median and IQR. For instance: NoRidge, NWAmes, and Greens have the highest medians while Landmrk and Blueste are the lowest. There are a decent amount of outliers (outside 1.5 * IQR upper bound or lower bound).

#### House Style  
1 Story Houses have the highest spread, with a ton of outliers. 2 Story houses unsurprisingly have the highest median, but 1.5 story (e.g. small loft) houses surprisingly have lower medians than both.

#### Building Type  
Single Family detached homes have the highest ceiling, but townhomes command a higher median price.

#### Sale Condition  
Homes sold under “Partial” and “Normal” conditions tend to have significantly higher median prices than those sold as “Abnormal” or “Family” sales.

#### Sale Type  
“New” sales show the highest median prices, while “COD” and “Con” types (contract for deed) trend lower and have less variability.

#### Garage Type  
“Built-in” and “Attached” garages correlate with higher median sale prices, while “None” or “Basement” garages tend to depress sale value.

#### Garage Finish  
Homes with a “Finished” or “Rough Finished” garage exhibit higher median prices, suggesting interior garage condition adds value.

#### Garage Quality  
Despite some noise, “Ex” and “Gd” garage quality levels trend upward in price, but category frequency is sparse.

#### Garage Condition  
Similar to garage quality, “TA” and better garage conditions command modestly higher prices, though variance is high.

#### Kitchen Quality  
A very strong ordinal relationship exists — better kitchen quality (Excellent, Good) strongly predicts higher sale price with tight IQRs.

#### Functional  
Homes marked “Typ” (typical functionality) strongly outperform other classes in median price, with steep drop-offs for anything less.

#### Heating QC  
Higher heating quality ratings align with higher prices, but low-frequency categories and overlapping distributions make this a weaker predictor.

#### Basement Quality  
“Ex” and “Gd” basement quality clearly separate themselves in sale price, reaffirming its high Pearson correlation.

#### Basement Condition  
Lower conditions (Po, Fa) suppress price noticeably, though high overlap among mid-quality levels may weaken predictive power.

#### Foundation  
“PConc” foundations stand out with the highest median sale price, suggesting poured concrete has premium value.

#### BsmtFin Type 1  
Finished basements (“GLQ”, “ALQ”) correlate with stronger home values, while unfinished basements or rec rooms perform worse.

#### BsmtFin Type 2  
Less informative overall; category distributions largely overlap and median prices remain similar.

#### MS SubClass  
There is wide variability, but detached newer homes and two-story dwellings command the highest prices.

#### Exterior 1st  
“CemntBd” and “BrickFace” exteriors trend higher, while cheaper materials like “AsbShng” and “WdShng” cluster lower.

#### Exterior 2nd  
Similar to Exterior 1st, but with more noise; still, “CmentBd” and “VinylSd” maintain decent medians.

#### Mas Vnr Type  
Stone veneer homes have substantially higher median sale prices; “None” and “BrkFace” rank lower.

#### Exter Qual  
Very strong ordinal pattern — homes with “Ex” or “Gd” exterior quality are much more valuable.

#### Exter Cond  
Surprisingly weak separation between categories; condition ratings don’t align cleanly with sale price.

#### Roof Style  
“Hipped” roofs slightly outperform others, but overall influence on sale price appears modest.

#### Lot Shape  
Regular lots tend to perform better than irregularly shaped ones, especially compared to “IR3”.

#### Lot Config  
“Cul-de-sac” and “Inside” lots have stronger performance, while corner and FR2 lots show lower medians.

#### Condition 1  
Homes adjacent to parks or greenbelts (e.g., “PosN”, “PosA”) outperform others substantially in sale price.

#### Bsmt Exposure  
Walkout and garden-level basements (“Gd”, “Av”) command premium prices over no exposure or minimal exposure.

#### Fireplace Quality  
Homes with “Ex” or “Gd” fireplace quality strongly outperform those with missing or poor ratings.

### Ad-Hoc Review of Suspect Columns

**Result**: majority of the suspect columns will be dropped, with the exception of Fireplace Quality, which the absence of a value indicates no fireplaces present in the sold home. This warrants further analysis.
"""

# Just under 50% of values for Fireplace Quality are missing. View counts of records by quality.
display(df['Fireplace Qu'].value_counts(dropna=False))
display(df['Street'].value_counts(dropna=False)) #drop street
display(df['Heating'].value_counts(dropna=False)) #drop heating
display(df['Roof Matl'].value_counts(dropna=False)) #drop roof matl
display(df['Electrical'].value_counts(dropna=False)) #drop electrical
display(df['Utilities'].value_counts(dropna=False)) #drop utilities
display(df['Alley'].value_counts(dropna=False)) #drop alley
display(df['Fence'].value_counts(dropna=False)) #drop fence

numeric_cols = df.select_dtypes(include=np.number)
missing_numeric = numeric_cols.isnull().sum()

print("Missing values in numeric columns:")
print(missing_numeric[missing_numeric > 0])

"""## Data Enrichment and Engineering

**Result**: there is clear evidence that several engineered features, such as `Total SF` (Total Square Feet) and subsequently `Qual x SF` (Overall Quality multiplied by Total Square Feet), will be of critical importance during the modeling portion of this project. Any engineered feature with a small Pearson correlation coefficient is dropped.

Final decision and decision justification can be viewed in the [data dictionary](https://docs.google.com/spreadsheets/d/1zRmdRlc2efk0RiQ3xv9OARQlnGgF3sbfwbhnpNbcv7Y/edit?usp=sharing).

### Engineered Features

#### Age

1.   Age of Home at Time of Sale (Yr Sold - Yr Built)
1.   Time Since Remodel (Yr Sold - Yr Remod or Add)
1.   Age Bucket

#### Size and Quality

1.   Total Square Feet
1.   Total Square Feet + Garage
1.   Total Baths (Full Bath + 0.5 * Half Bath)
1.   Number of Stories
1.   Price per Square Feet
1.   Overall Quality x Size

#### Binary Features
1.   Has Basement (0, 1)
1.   Has Central Air (0, 1)
1.   Has Fireplace (0, 1)
1.   Has Porch (0, 1)
1.   Has Deck (0, 1)
1.   Has Garage (0, 1)
1.   Has Been Remodeled (0, 1), If Year Remod / Add > Year Build then 1 else 0

#### Time

1.   Season Sold (calculated by Mo Sold)
"""

# Age
df['House Age'] = df['Yr Sold'] - df['Year Built']
df['Remodel Age'] = df['Yr Sold'] - df['Year Remod/Add']
df['Age Bucket'] = pd.cut(df['House Age'], bins=[-1,10,30,60,300],
                          labels=['≤10','11-30','30-60','60+'])

# Size and Quality
df['Total SF'] = df['Total Bsmt SF'] + df['1st Flr SF'] + df['2nd Flr SF']
df['Total SF Plus Garage'] = df['Total SF'] + df['Garage Area']
df['Total Baths'] = df['Full Bath'] + 0.5 * df['Half Bath']
df['Number of Stories'] = df['2nd Flr SF'].apply(lambda x: 2 if x > 0 else 1)
df['Price per SF'] = df['SalePrice'] / df['Total SF']
df['Qual x SF'] = df['Overall Qual'] * df['Total SF']

# Binary Features
df['Has Basement'] = df['Bsmt Qual'].notna().astype(int)
df['Has Central Air'] = df['Central Air'].map({'Y': 1, 'N': 0})
df['Has Pool'] = df['Pool Area'].apply(lambda x: 1 if x > 0 else 0)
df['Has Fireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)

total_porch_area = df['Open Porch SF'] + df['Enclosed Porch'] + df['3Ssn Porch']
+ df['Screen Porch']

df['Has Porch'] = total_porch_area.apply(lambda x: 1 if x > 0 else 0)
df['Has Deck'] = df['Wood Deck SF'].apply(lambda x: 1 if x > 0 else 0)
df['Has Garage'] = df['Garage Area'].apply(lambda x: 1 if x > 0 else 0)
df['Has Remodeled'] = df.apply(lambda row: 1 if row['Year Remod/Add'] > row['Year Built'] else 0, axis=1)

# Multiplicative


# Time
df['Season Sold'] = df['Mo Sold'].apply(lambda x: 'Winter' if x in [12, 1, 2]
                                        else 'Spring' if x in [3, 4, 5]
                                        else 'Summer' if x in [6, 7, 8]
                                        else 'Fall')

# Age
correlation_house_age = df['House Age'].corr(df['SalePrice'])
correlation_remodel_age = df['Remodel Age'].corr(df['SalePrice'])

print("Correlation with 'House Age':", correlation_house_age)
print("Correlation with 'Remodel Age':", correlation_remodel_age)

# Size and Quality
correlation_total_sf = df['Total SF'].corr(df['SalePrice'])
correlation_total_sf_plus_garage = df['Total SF Plus Garage'].corr(df['SalePrice'])
correlation_total_baths = df['Total Baths'].corr(df['SalePrice'])
correlation_number_of_stories = df['Number of Stories'].corr(df['SalePrice'])
correlation_qual_x_sf = df['Qual x SF'].corr(df['SalePrice'])
correlation_total_bedrooms = df['Bedroom AbvGr'].corr(df['SalePrice']) #drop
correlation_total_rooms = df['TotRms AbvGrd'].corr(df['SalePrice'])
correlation_garage_cars = df['Garage Cars'].corr(df['SalePrice'])
correlation_overall_qual = df['Overall Qual'].corr(df['SalePrice'])
correlation_overall_cond = df['Overall Cond'].corr(df['SalePrice']) #drop
correlation_lot_area = df['Lot Area'].corr(df['SalePrice']) #drop
correlation_gr_liv_area = df['Gr Liv Area'].corr(df['SalePrice'])
correlation_garage_area = df['Garage Area'].corr(df['SalePrice'])
correlation_wood_deck_sf = df['Wood Deck SF'].corr(df['SalePrice']) #drop

print("Correlation with 'Total SF':", correlation_total_sf)
print("Correlation with 'Total SF Plus Garage':", correlation_total_sf_plus_garage)
print("Correlation with 'Total Baths':", correlation_total_baths)
print("Correlation with 'Number of Stories':", correlation_number_of_stories)
print("Correlation with 'Qual x SF':", correlation_qual_x_sf)
print("Correlation with 'Total Bedrooms':", correlation_total_bedrooms)
print("Correlation with 'Total Rooms':", correlation_total_rooms)
print("Correlation with 'Garage Cars':", correlation_garage_cars)
print("Correlation with 'Overall Qual':", correlation_overall_qual)
print("Correlation with 'Overall Cond':", correlation_overall_cond)
print("Correlation with 'Lot Area':", correlation_lot_area)
print("Correlation with 'Gr Liv Area':", correlation_gr_liv_area)
print("Correlation with 'Garage Area':", correlation_garage_area)
print("Correlation with 'Wood Deck SF':", correlation_wood_deck_sf)

# Binary Features
correlation_has_basement = df['Has Basement'].corr(df['SalePrice'])
correlation_central_air = df['Has Central Air'].corr(df['SalePrice']) #drop
correlation_has_pool = df['Has Pool'].corr(df['SalePrice']) #drop
correlation_has_fireplace = df['Has Fireplace'].corr(df['SalePrice'])
correlation_has_porch = df['Has Porch'].corr(df['SalePrice'])
correlation_has_deck = df['Has Deck'].corr(df['SalePrice'])
correlation_has_garage = df['Has Garage'].corr(df['SalePrice'])
correlation_has_remodeled = df['Has Remodeled'].corr(df['SalePrice'])

print("Correlation with 'Has Basement':", correlation_has_basement)
print("Correlation with 'Has Central Air':", correlation_central_air)
print("Correlation with 'Has Pool':", correlation_has_pool)
print("Correlation with 'Has Fireplace':", correlation_has_fireplace)
print("Correlation with 'Has Porch':", correlation_has_porch)
print("Correlation with 'Has Deck':", correlation_has_deck)
print("Correlation with 'Has Garage':", correlation_has_garage)
print("Correlation with 'Has Remodeled':", correlation_has_remodeled)

# correlation between all numerical features and SalePrice, excluding objects
numeric_cols = df.select_dtypes(include=np.number).columns
corr_matrix = df[numeric_cols].corr()
corr_matrix['SalePrice'].sort_values(ascending=False)

# Look at multiplicatives of the engineered binary features, checking for
# pearson correlation coefficient.
from itertools import combinations

binary_features = [
    'Has Basement',
    'Has Central Air',
    'Has Pool',
    'Has Fireplace',
    'Has Porch',
    'Has Deck',
    'Has Garage',
    'Has Remodeled'
]

# Create a dictionary to store the correlations
combination_correlations = {}

# Iterate through all two-pair combinations
for feature1, feature2 in combinations(binary_features, 2):
    # Create the combined feature
    combined_feature_name = f'{feature1}_and_{feature2}'
    df[combined_feature_name] = df[feature1] * df[feature2]

    # Calculate the Pearson correlation coefficient with 'SalePrice'
    correlation = df[combined_feature_name].corr(df['SalePrice'])

    # Store the correlation
    combination_correlations[combined_feature_name] = correlation

# Print the correlations
print("Pearson Correlation Coefficients for Two-Pair Combinations of Engineered Binary Features:")
for combo, corr in combination_correlations.items():
    print(f"{combo}: {corr:.4f}")

# Iteratively drop the combined correlation features. No real added value.
for combo, corr in combination_correlations.items():
    df.drop(columns=[combo], inplace=True)

# drop values where Total SF > 6000 (3 total outliers)
df.drop(df[df['Total SF'] > 6000].index, inplace=True)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))


axes[0, 0].scatter(df['Qual x SF'], df['SalePrice'])
axes[0, 0].set_title('Quality x Square Foot vs. Sale Price')
axes[0, 0].set_xlabel('Quality x Square Feet')
axes[0, 0].set_ylabel('Sale Price')


axes[0, 1].scatter(df['Total SF'], df['SalePrice'])
axes[0, 1].set_title('Total SF vs. Sale Price')
axes[0, 1].set_xlabel('Total SF')
axes[0, 1].set_ylabel('Sale Price')

axes[1, 0].scatter(df['Total SF'], df['SalePrice'])
axes[1, 0].set_title('Total SF vs. Sale Price (Outliers Removed)')
axes[1, 0].set_xlabel('Total SF')
axes[1, 0].set_ylabel('Sale Price')


axes[1, 1].scatter(df['Overall Qual'], df['SalePrice'])
axes[1, 1].set_title('Overall Quality vs. Sale Price')
axes[1, 1].set_xlabel('Overall Quality')
axes[1, 1].set_ylabel('Sale Price')

plt.tight_layout()
plt.show()

# Encode
ordinal_map = {
    'Ex': 5,
    'Gd': 4,
    'TA': 3,
    'Fa': 2,
    'Po': 1,
    'NA': 0,
    'Fin': 3,
    'Unf': 2,
    'RFn': 1,
    'Y': 1,
    'N': 0,
    'Typ': 8,
    'Min1': 7,
    'Min2': 6,
    'Mod': 5,
    'Maj1': 4,
    'Maj2': 3,
    'Sev': 2,
    'Sal': 1,
    'GdPrv': 2,
    'MnPrv': 1,
    'GdWo': 2,
    'MnWw': 1
}

for col in ['Exter Qual', 'Exter Cond', 'Bsmt Qual', 'Bsmt Cond', 'Heating QC',
            'Kitchen Qual', 'Fireplace Qu', 'Garage Finish','Functional',
            'Garage Qual', 'Garage Cond', 'Pool QC', 'Fence']:
    df[col+'_Ord'] = df[col].map(ordinal_map).fillna(0)

# Correlation of SalePrice and Ordinal Map Encoded Features
ord_cols = [col for col in df.columns if '_Ord' in col]
corr_matrix = df[ord_cols + ['SalePrice']].corr()
corr_matrix['SalePrice'].sort_values(ascending=False)

# One Hot Encoding Nominal Variables
nominal_cols = [
    'Neighborhood','House Style','Bldg Type','Sale Condition','Sale Type',
    'Garage Type','Foundation','Exterior 1st','Exterior 2nd',
    'Roof Style','MS Zoning','Lot Shape','Lot Config','Condition 1'
]

# Any value that has an occurrence of less than 30, categorize as "Other"
def bundle_rare(s, min_count=30):
    vc = s.value_counts()
    return s.where(s.isin(vc[vc >= min_count].index), 'Other')

df_nom = df[nominal_cols].apply(bundle_rare)

dummies = pd.get_dummies(df_nom, drop_first=True, prefix_sep='__')
df = pd.concat([df, dummies], axis=1)

# Check dummy variables for point-biserial correlation
from scipy.stats import pointbiserialr

keep_dummy, drop_dummy = [], []

for col in dummies.columns:
    r, p = pointbiserialr(df[col], df['SalePrice'])
    if abs(r) >= 0.05 and p < 0.05:
        keep_dummy.append(col)
    else:
        drop_dummy.append(col)

print("Keep:", keep_dummy)
print("Drop:", drop_dummy)

"""## Final Output

Once we have one hot encoded our categorical features and screened for statistical significance, we will drop the features that do not meet our bar for further analysis. These again are:

* Pearson correlation coefficient |r| < 0.5 for both raw numerical features and engineered numerical features
* Pearson correlation coefficient |r| < 0.5 for ordinally mapped categorical features (e.g. Yes = 1, No = 0, Excellent = 5, None = 0 etc.)
* Missing values > 60%
* Redundant, almost constant, features with > 90% of the data corresponds to only 1 value.

There are a few exceptions, e.g. "Fireplace Quality" whose absence could be a useful signal in the models that will be evaluated in Phase 2 and Phase 3 of the project.
"""

# Create final dataframe prepping for final output, and drop all columns that do
# not meet our heuristics

df_cleaned = df.drop(columns=['Order', 'PID', 'Street', 'Heating', 'Roof Matl',
                 'Electrical', 'Utilities', 'Alley', 'MS Zoning',
                  'Lot Frontage', 'Lot Area', 'Lot Shape', 'Land Contour',
                  'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',
                  'Condition 2', 'Bldg Type', 'House Style', 'Overall Cond',
                  'Roof Style', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',
                  'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation',
                  'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin SF 1',
                  'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Heating QC',
                  'Central Air', '2nd Flr SF', 'Low Qual Fin SF',
                  'Bsmt Full Bath', 'Bsmt Half Bath', 'Half Bath',
                  'Bedroom AbvGr', 'Kitchen AbvGr', 'Kitchen Qual',
                  'TotRms AbvGrd', 'Functional', 'Fireplaces', 'Fireplace Qu',
                  'Garage Type', 'Garage Qual', 'Garage Cond', 'Paved Drive',
                  'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch',
                  '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',
                  'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold',
                  'Sale Type', 'Sale Condition', 'Number of Stories',
                  'Has Basement', 'Has Central Air', 'Has Pool',
                  'Has Fireplace', 'Has Porch', 'Has Deck', 'Has Garage',
                  'Has Remodeled', 'Exter Cond_Ord', 'Bsmt Cond_Ord',
                  'Garage Finish_Ord', 'Garage Qual_Ord', 'Garage Cond_Ord',
                  'Pool QC_Ord', 'Fence_Ord'])

df_cleaned = df_cleaned.drop(columns=drop_dummy)

df_cleaned.head()

"""## Conducting Early Trials

First, we will use a single linear regression model to evaluate the Quality x Square Feet feature. We do not anticipate this will be as strong as multiple linear regression or classifier models.
"""

# conduct early trial, reviewing
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# drop values where Total SF > 6000 (3 total outliers)
df_cleaned.drop(df_cleaned[df_cleaned['Total SF'] > 6000].index, inplace=True)

# drop values where Qual x SF is NaN (one record)
df_cleaned.dropna(subset=['Qual x SF'], inplace=True)

# Set seed
seed = 123

# Percent of data reserved for test set
test_p = 0.20

# Set the features
X = df_cleaned[['Qual x SF']]
y = df_cleaned['SalePrice']

# Define training and test data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_p, random_state=seed
)

# Plot training and regression
plt = sns.regplot(x=X_train, y=y_train, ci=False, line_kws={'color': 'red'})
plt.set_title('Quality x Square Feet vs. Sale Price')
plt.set_xlabel('Quality x Square Feet')
plt.set_ylabel('Sale Price')

# Initialize linear regression model
model = LinearRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Print model coefficients
print("Model Coefficients:")
print(model.coef_)
print("Model Intercept:")
print(model.intercept_)

# Regression w/ training data
y_train_pred = model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
mae_train = mean_absolute_error(y_train, y_train_pred)

# Print the results
print("Training Results:")
print(f"Mean Squared Error: {mse_train}")
print(f"R-squared: {r2_train}")
print(f"Mean Absolute Error: {mae_train}")

# Regression w/ test data
y_test_pred = model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)

# Print the results
print("Test Results:")
print(f"Mean Squared Error: {mse_test}")
print(f"R-squared: {r2_test}")
print(f"Mean Absolute Error: {mae_test}")

# Plot test and train using subplots
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Use model coefficient and y-intercept to define formula for yhat
yhat_train = model.coef_[0] * X_train + model.intercept_

# Plot training
axes[0].scatter(X_train, y_train, color='blue', label='Training Data')
axes[0].plot(X_train, yhat_train, color='red', label='Regression Line')
axes[0].set_title('Training Data')
axes[0].set_xlabel('Quality x Square Feet')
axes[0].set_ylabel('Sale Price')
axes[0].legend()

# Plot test
yhat_test = model.coef_[0] * X_test + model.intercept_
axes[1].scatter(X_test, y_test, color='blue', label='Test Data')
axes[1].plot(X_test, yhat_test, color='red', label='Regression Line')
axes[1].set_title('Test Data')
axes[1].set_xlabel('Quality x Square Feet')
axes[1].set_ylabel('Sale Price')
axes[1].legend()

plt.tight_layout()
plt.show()

# export df_cleaned to csv
df_cleaned.to_csv('ACS-5113_Petabyte_Pirates_Deliverable_1.csv', index=False)